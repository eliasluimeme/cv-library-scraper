# CV-Library Scraper

A robust Python web scraper for CV-Library's recruiter portal that can authenticate, search for CVs using predefined keywords, process results, and download CVs based on specified quantities.

## ğŸš€ Project Status

**Current Implementation Status: Phase 1-2 COMPLETE âœ… | Phase 3 In Progress ğŸš§**

### âœ… **COMPLETED FEATURES (Phase 1-2)**

#### 1. **Project Structure & Configuration** âœ…
- âœ… Complete directory structure with proper Python packages
- âœ… Comprehensive configuration management (YAML + environment variables)
- âœ… Logging system with file rotation and multiple handlers
- âœ… Settings validation and error handling
- âœ… Environment variable management with .env support

#### 2. **Data Models** âœ…
- âœ… CV and candidate information models with full serialization
- âœ… Search result collections with filtering and sorting capabilities
- âœ… Data validation and type safety with dataclasses
- âœ… Duplicate detection algorithms
- âœ… File naming conventions and metadata tracking

#### 3. **Utility Framework** âœ…
- âœ… Rate limiting and respectful scraping utilities
- âœ… File operations and safe filename generation
- âœ… WebDriver utilities with retry logic
- âœ… Session management and persistence
- âœ… Data validation and text processing helpers
- âœ… Progress tracking and statistics utilities

#### 4. **CLI Interface** âœ…
- âœ… Complete command-line interface with comprehensive options
- âœ… Configuration override capabilities
- âœ… Session management and resumption
- âœ… Dry-run mode for testing
- âœ… Multiple output formats and logging levels
- âœ… Comprehensive help and validation

#### 5. **Core Architecture** âœ…
- âœ… Main `CVLibraryScraper` class with workflow coordination
- âœ… Modular architecture with separate managers for auth/search/download
- âœ… Session tracking and persistence (JSON-based)
- âœ… Comprehensive error handling and logging
- âœ… Resource cleanup and graceful shutdown

### ğŸš§ **IN PROGRESS (Phase 3)**

#### Authentication System ğŸ”„
- ğŸš§ CV-Library login form detection and interaction
- ğŸš§ Session cookie management and persistence
- ğŸš§ Login verification and retry logic

#### Search Functionality ğŸ”„
- ğŸš§ Search form interaction with CV-Library portal
- ğŸš§ Result parsing and pagination handling
- ğŸš§ Advanced filtering and keyword matching

#### Download System ğŸ”„
- ğŸš§ CV download queue management
- ğŸš§ File format handling and organization
- ğŸš§ Progress tracking and resume capability

## ğŸ“‹ Current Working Features

### **Configuration Management**
- YAML and environment variable configuration âœ…
- Multi-source configuration loading âœ…
- Configuration validation and error reporting âœ…

### **Logging System**
- Comprehensive logging with file rotation âœ…
- Multiple log levels and handlers âœ…
- Separate error logging âœ…

### **Session Management**
- Save and resume scraping sessions âœ…
- Session data persistence in JSON format âœ…
- Automatic session ID generation âœ…

### **CLI Interface**
- Full command-line interface with argument parsing âœ…
- Configuration override capabilities âœ…
- Dry-run mode for testing configurations âœ…

### **Data Handling**
- Structured data models for CVs and search results âœ…
- Data serialization and validation âœ…
- File utilities and naming conventions âœ…

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.8 or higher (tested with Python 3.13)
- Google Chrome or Firefox browser
- CV-Library recruiter account

### Setup

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd cv-library-scraper
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment variables**
   ```bash
   # Copy the example environment file
   cp env_example.txt .env
   
   # Edit .env with your credentials
   CV_LIBRARY_USERNAME=your_username
   CV_LIBRARY_PASSWORD=your_password
   ```

5. **Test the installation**
   ```bash
   python main.py --help
   ```

## ğŸ“– Usage

### **Working Commands (Phase 1-2)**

#### Basic Testing
```bash
# Test configuration and CLI
python main.py --help

# Dry run to test configuration
python main.py --keywords "Python developer" --location "London" --dry-run

# Test with configuration file
python main.py --config config/config.yaml --dry-run
```

#### Session Management
```bash
# Run with session saving
python main.py --keywords "Data scientist" --save-session

# Resume from saved session (when Phase 3 is complete)
python main.py --resume-session sessions/session_20250731_180307_20jcvs.json
```

#### Advanced Configuration
```bash
# Custom output directory and file formats
python main.py --keywords "DevOps" --output-dir ./downloads/ --file-formats "pdf,docx"

# Debug mode with custom delays
python main.py --keywords "React" --log-level DEBUG --delay-min 3 --delay-max 7

# Different browser settings
python main.py --keywords "Machine Learning" --headless false --browser chrome
```

### **Example Output**

```bash
$ python main.py --keywords "Python developer" --location "London" --quantity 5 --dry-run

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    CV-Library Scraper v1.0                   â•‘
â•‘         Automated CV downloading from CV-Library portal       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‹ Configuration Summary:
   Search Keywords: Python developer
   Search Locations: London
   Download Quantity: 5
   Download Path: ./downloaded_cvs/
   Browser: chrome (headless: True)
   Rate Limiting: 2-5s delays

ğŸš€ CV-Library Scraper initialized successfully!
   ğŸ” Dry run mode - no actual scraping will be performed

âœ… All systems ready! The scraper would now:
   1. ğŸ” Authenticate with CV-Library
   2. ğŸ” Search for CVs matching criteria
   3. ğŸ“„ Parse and filter results
   4. â¬‡ï¸  Download selected CVs
   5. ğŸ“Š Generate reports
```

## âš™ï¸ Configuration

### Environment Variables (.env)
```bash
# CV-Library Credentials
CV_LIBRARY_USERNAME=your_username
CV_LIBRARY_PASSWORD=your_password

# Download Configuration
DOWNLOAD_PATH=./downloaded_cvs/
MAX_DOWNLOADS_PER_SESSION=50

# Browser Settings
BROWSER=chrome
HEADLESS=true
BROWSER_TIMEOUT=30

# Rate Limiting
DELAY_MIN_SECONDS=2
DELAY_MAX_SECONDS=5
REQUESTS_PER_MINUTE=10
```

### Configuration File (config/config.yaml)
```yaml
search_criteria:
  keywords:
    - "Python developer"
    - "Data scientist"
  locations:
    - "London"
    - "Manchester"
  salary_range:
    min: 30000
    max: 80000

download_settings:
  max_quantity: 100
  file_formats: ["pdf", "doc", "docx"]
  organize_by_keywords: true
```

## ğŸ“ Current Project Structure

```
cv-library-scraper/
â”œâ”€â”€ src/                          # Source code âœ…
â”‚   â”œâ”€â”€ __init__.py              # Package initialization âœ…
â”‚   â”œâ”€â”€ config/                   # Configuration management âœ…
â”‚   â”‚   â”œâ”€â”€ __init__.py          # Config package âœ…
â”‚   â”‚   â”œâ”€â”€ settings.py          # Settings classes âœ…
â”‚   â”‚   â””â”€â”€ config_loader.py     # Configuration loader âœ…
â”‚   â”œâ”€â”€ models/                   # Data models âœ…
â”‚   â”‚   â”œâ”€â”€ __init__.py          # Models package âœ…
â”‚   â”‚   â”œâ”€â”€ cv_data.py           # CV and candidate models âœ…
â”‚   â”‚   â””â”€â”€ search_result.py     # Search result models âœ…
â”‚   â””â”€â”€ scraper/                  # Core scraping functionality âœ…
â”‚       â”œâ”€â”€ __init__.py          # Scraper package âœ…
â”‚       â”œâ”€â”€ cv_library_scraper.py # Main scraper class âœ…
â”‚       â”œâ”€â”€ auth.py              # Authentication [Phase 3] ğŸš§
â”‚       â”œâ”€â”€ search.py            # Search functionality [Phase 3] ğŸš§
â”‚       â”œâ”€â”€ download.py          # Download management [Phase 3] ğŸš§
â”‚       â””â”€â”€ utils.py             # Utility functions âœ…
â”œâ”€â”€ config/                       # Configuration files âœ…
â”‚   â”œâ”€â”€ config.yaml              # Main configuration âœ…
â”‚   â””â”€â”€ logging_config.yaml      # Logging configuration âœ…
â”œâ”€â”€ downloaded_cvs/               # Downloaded CV files âœ…
â”œâ”€â”€ logs/                         # Log files âœ…
â”‚   â”œâ”€â”€ cv_scraper.log           # Main log file âœ…
â”‚   â””â”€â”€ cv_scraper_errors.log    # Error log file âœ…
â”œâ”€â”€ sessions/                     # Session data âœ…
â”‚   â””â”€â”€ session_*.json           # Saved session files âœ…
â”œâ”€â”€ tests/                        # Test files [Future]
â”œâ”€â”€ requirements.txt              # Python dependencies âœ…
â”œâ”€â”€ main.py                       # CLI entry point âœ…
â”œâ”€â”€ env_example.txt              # Environment template âœ…
â””â”€â”€ README.md                     # Documentation âœ…
```

## ğŸ§ª Testing Current Features

```bash
# Test configuration loading
python -c "from src.config import ConfigLoader; print('âœ… Config loading works')"

# Test data models
python -c "from src.models import CVData; print('âœ… Data models work')"

# Test CLI with different options
python main.py --keywords "test" --dry-run --log-level DEBUG

# Test session management
python main.py --keywords "Python" --save-session
ls sessions/  # Check for saved session file
```

## ğŸ“Š Logging

The scraper provides comprehensive logging:

- **Console Output**: Real-time progress and status updates âœ…
- **File Logging**: Detailed logs saved to `logs/cv_scraper.log` âœ…
- **Error Logging**: Separate error log at `logs/cv_scraper_errors.log` âœ…
- **Log Rotation**: Automatic log file rotation (10MB max, 5 backups) âœ…

Log levels: `DEBUG`, `INFO`, `WARNING`, `ERROR`

## ğŸ”’ Security & Ethics

### Implemented Safeguards âœ…
- **Rate Limiting**: Configurable delays between requests
- **Respectful Scraping**: Built-in robots.txt compliance
- **Secure Credentials**: Environment variable storage
- **Session Management**: Resumable sessions to avoid re-scraping

## ğŸ—ºï¸ Development Roadmap

### âœ… **Phase 1-2: Foundation (COMPLETE)**
- [x] Project structure and configuration management
- [x] Data models and utilities
- [x] CLI interface and session management
- [x] Logging and error handling
- [x] Rate limiting and ethical scraping framework

### ğŸš§ **Phase 3: Core Scraping (IN PROGRESS)**
- [ ] Authentication system implementation
- [ ] Basic search functionality
- [ ] CV preview parsing
- [ ] Simple download capability

### ğŸ“‹ **Phase 4: Enhanced Features (PLANNED)**
- [ ] Advanced search filters
- [ ] Pagination handling
- [ ] Duplicate detection
- [ ] Progress tracking

### ğŸ”® **Phase 5: Advanced Features (FUTURE)**
- [ ] GUI interface
- [ ] Database integration
- [ ] Email notifications
- [ ] Scheduling capabilities

## ğŸ¯ **Current Implementation Status**

- **Foundation**: 100% Complete âœ…
- **Configuration**: 100% Complete âœ…  
- **CLI Interface**: 100% Complete âœ…
- **Data Models**: 100% Complete âœ…
- **Session Management**: 100% Complete âœ…
- **Authentication**: 0% (Phase 3 Next) ğŸš§
- **Search**: 0% (Phase 3 Next) ğŸš§
- **Download**: 0% (Phase 3 Next) ğŸš§

---

**Ready for Phase 3!** The foundation is solid and all core infrastructure is in place. Next step: Implement CV-Library authentication, search, and download functionality. 